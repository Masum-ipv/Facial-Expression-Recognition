{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Facial Expression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hrgXg7NRx-Zj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load necessary packages"
      ]
    },
    {
      "metadata": {
        "id": "j2IYR3YYxSGx",
        "colab_type": "code",
        "outputId": "9a85d609-c176-4b0c-d8a8-90a54e1648d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "import csv\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, Conv2D, Input, Dense, Dropout, MaxPool2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import callbacks\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.optimizers import Adamax\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "result = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CP8atr7CgiNG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download Training Data from Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "XdGLCqkYCu-j",
        "colab_type": "code",
        "outputId": "c9cd6768-ba6e-4d72-e474-60cfeb56a228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "download = drive.CreateFile({'id': '1wn1WdiMqfaBkHjLGgAGo7mgB2iCCpabk'})  #From shareable link\n",
        "download.GetContentFile('fer2013.csv')\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  fer2013.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ys09NLmiftW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load data-set and splite into training-validation and test dataset"
      ]
    },
    {
      "metadata": {
        "id": "OvpCMelfx9zT",
        "colab_type": "code",
        "outputId": "9a43c9a6-cf3b-494a-90b1-4b4eb50fe230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "\ttrain_data_X, train_data_Y = [], []\n",
        "\tvalidation_data_X, validation_data_Y = [], []\n",
        "\ttest_data_X, test_data_Y = [], []\n",
        "\twith open(\"fer2013.csv\", 'r') as csvfile:\n",
        "\t\tdatareader = csv.reader(csvfile, delimiter =',')\n",
        "\t\theaders = next(datareader)\n",
        "\t\tprint(headers) \n",
        "\t\tfor row in datareader:\n",
        "\t\t\temotion = int(row[0])\n",
        "\t\t\tusage = row[2]\n",
        "\t\t\tpixels = [int(p) for p in row[1].split()]\n",
        "\t\t\tif usage == \"Training\":\n",
        "\t\t\t\ttrain_data_X.append(pixels)\n",
        "\t\t\t\ttrain_data_Y.append(emotion)\n",
        "\t\t\telif usage == \"PrivateTest\":\n",
        "\t\t\t\tvalidation_data_X.append(pixels)\n",
        "\t\t\t\tvalidation_data_Y.append(emotion)\n",
        "\t\t\telse:\n",
        "\t\t\t\ttest_data_X.append(pixels)\n",
        "\t\t\t\ttest_data_Y.append(emotion)\n",
        "\ttrain_data_X, train_data_Y = np.array(train_data_X)/255.0, np.array(train_data_Y)\n",
        "\tvalidation_data_X, validation_data_Y = np.array(validation_data_X)/255.0, np.array(validation_data_Y)\n",
        "\ttest_data_X, test_data_Y = np.array(test_data_X)/255.0, np.array(test_data_Y)\n",
        "\n",
        "  \n",
        "\tprint(\"train_data \", train_data_X.shape)\n",
        "\tprint(\"validation_data \", validation_data_X.shape)\n",
        "\tprint(\"test_data \", test_data_X.shape)\n",
        "\treturn train_data_X, train_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y\n",
        "\n",
        "train_data_X, train_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y = load_data()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['emotion', 'pixels', 'Usage']\n",
            "train_data  (28709, 2304)\n",
            "validation_data  (3589, 2304)\n",
            "test_data  (3589, 2304)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QTTinxHEyGyj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "VGG-16 model"
      ]
    },
    {
      "metadata": {
        "id": "--S8Vyw_1HK3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "# Reshape array\n",
        "train_data_X = train_data_X.reshape(28709, 48, 48, 1)\n",
        "validation_data_X = validation_data_X.reshape(3589, 48, 48, 1)\n",
        "test_data_X = test_data_X.reshape(3589, 48, 48, 1)\n",
        "\n",
        "train_data_Y = to_categorical(train_data_Y, num_classes=7)\n",
        "validation_data_Y = to_categorical(validation_data_Y, num_classes=7)\n",
        "test_data_Y = to_categorical(test_data_Y, num_classes=7)\n",
        "\n",
        "print(\"train_data \", train_data_X.shape)\n",
        "print(\"train_data \", train_data_Y.shape)\n",
        "print(\"validation_data \", validation_data_X.shape)\n",
        "print(\"validation_data \", validation_data_Y.shape)\n",
        "print(\"test_data \", test_data_X.shape)\n",
        "print(\"test_data \", test_data_Y.shape)\n",
        "\n",
        "# keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "model = Sequential()\n",
        "\n",
        "model.add(ZeroPadding2D((1,1), input_shape=(48, 48, 1)))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "# FC layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# start training the model\n",
        "hist = model.fit(train_data_X, train_data_Y, batch_size=128, epochs=20, verbose=1, validation_data=(validation_data_X, validation_data_Y), shuffle=True)\n",
        "\n",
        "# save the current model and weights\n",
        "#model.save_weights('weights.h5')\n",
        "model.save('model.h5')\n",
        "\n",
        "loss, acc = model.evaluate(test_data_X, test_data_Y, batch_size=64)\n",
        "\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Loss: \",loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kIHBfjB_l3aM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train on 28709 samples, validate on 3589 samples\n",
        "Epoch 1/25\n",
        "28709/28709 [==============================] - 76s 3ms/step - loss: 1.8285 - acc: 0.2639 - val_loss: 1.8375 - val_acc: 0.2497  \n",
        "Epoch 2/25\n",
        "28709/28709 [==============================] - 70s 2ms/step - loss: 1.5331 - acc: 0.3842 - val_loss: 1.5009 - val_acc: 0.4452  \n",
        "\n",
        "Epoch 24/25\n",
        "28709/28709 [==============================] - 70s 2ms/step - loss: 0.1793 - acc: 0.9454 - val_loss: 1.7357 - val_acc: 0.6266   \n",
        "Epoch 25/25\n",
        "28709/28709 [==============================] - 70s 2ms/step - loss: 0.1596 - acc: 0.9510 - val_loss: 2.0783 - val_acc: 0.6141  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "Test:  \n",
        "Accuracy:  0.606018389531848\n",
        "Loss:  2.216669141166753"
      ]
    },
    {
      "metadata": {
        "id": "ZX0cOtEDca6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save the current model and weights\n",
        "#model.save_weights('weights.h5')\n",
        "model.save('model.h5')\n",
        "\n",
        "loss, acc = model.evaluate(test_data_X, test_data_Y, batch_size=64)\n",
        "\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Loss: \",loss)\n",
        "\n",
        "!ls\n",
        "def load_image(img_path, show=True):\n",
        "\n",
        "    img = image.load_img(img_path, target_size=(48, 48), color_mode = \"grayscale\")\n",
        "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
        "    img_tensor /= 255.\n",
        "    #print(img_tensor.shape)\n",
        "    if show:\n",
        "        plt.imshow(img, cmap='gray')\n",
        "\n",
        "    return img_tensor\n",
        "    # show the image\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # load model\n",
        "    #model = load_model(\"model.h5\")\n",
        "\n",
        "    # image path\n",
        "    !wget https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyndHmil5Mdk8-v-6To_6ncm-l8-L5n0l0Bh5dcOR6ZUkq-pODmQ #Angry\n",
        "    !wget https://www.sciencedaily.com/images/2009/03/090312093916_1_540x360.jpg #Disgust\n",
        "    !wget https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcREqmMQoG0_0XOYf5YWhLIdo-xZmohGMOWm0P4aFUHpFzYiB_yY #Fear\n",
        "    !wget https://w3.chabad.org/media/images/689/tyZo6894268.jpg #Happy\n",
        "    !wget https://c1.staticflickr.com/4/3675/9436653177_fd00cc9d2c_b.jpg #Sad\n",
        "    !wget https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTt54l_3Sud9ektJH69BXBOADn3qk7d0XrTpjrAWJ6u5omaX2NlcQ # Surprise \n",
        "    !wget https://goldenmeancalipers.com/wp-content/uploads/2011/12/mirror11.jpg # Netural\n",
        "\n",
        "    # load a single image\n",
        "    new_image = load_image('mirror11.jpg')\n",
        "    \n",
        "    # check prediction\n",
        "    pred = model.predict(new_image)\n",
        "    index = pred.argmax()\n",
        "    print(result[index])\n",
        "    #print(max(pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}