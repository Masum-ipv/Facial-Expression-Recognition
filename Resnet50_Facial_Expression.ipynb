{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Facial Expression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hrgXg7NRx-Zj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load necessary packages"
      ]
    },
    {
      "metadata": {
        "id": "j2IYR3YYxSGx",
        "colab_type": "code",
        "outputId": "9a85d609-c176-4b0c-d8a8-90a54e1648d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "import csv\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, Conv2D, Input, Dense, Dropout, MaxPool2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import callbacks\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "from keras.optimizers import Adamax\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "result = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CP8atr7CgiNG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download Training Data from Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "XdGLCqkYCu-j",
        "colab_type": "code",
        "outputId": "c9cd6768-ba6e-4d72-e474-60cfeb56a228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "download = drive.CreateFile({'id': '1wn1WdiMqfaBkHjLGgAGo7mgB2iCCpabk'})  #From shareable link\n",
        "download.GetContentFile('fer2013.csv')\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  fer2013.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ys09NLmiftW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load data-set and splite into training-validation and test dataset"
      ]
    },
    {
      "metadata": {
        "id": "OvpCMelfx9zT",
        "colab_type": "code",
        "outputId": "3c1962ce-02ab-48da-cb81-acf19bda256d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "\ttrain_data_X, train_data_Y = [], []\n",
        "\tvalidation_data_X, validation_data_Y = [], []\n",
        "\ttest_data_X, test_data_Y = [], []\n",
        "\twith open(\"fer2013.csv\", 'r') as csvfile:\n",
        "\t\tdatareader = csv.reader(csvfile, delimiter =',')\n",
        "\t\theaders = next(datareader)\n",
        "\t\tprint(headers) \n",
        "\t\tfor row in datareader:\n",
        "\t\t\temotion = int(row[0])\n",
        "\t\t\tusage = row[2]\n",
        "\t\t\tpixels = [int(p) for p in row[1].split()]\n",
        "\t\t\tif usage == \"Training\":\n",
        "\t\t\t\ttrain_data_X.append(pixels)\n",
        "\t\t\t\ttrain_data_Y.append(emotion)\n",
        "\t\t\telif usage == \"PrivateTest\":\n",
        "\t\t\t\tvalidation_data_X.append(pixels)\n",
        "\t\t\t\tvalidation_data_Y.append(emotion)\n",
        "\t\t\telse:\n",
        "\t\t\t\ttest_data_X.append(pixels)\n",
        "\t\t\t\ttest_data_Y.append(emotion)\n",
        "\ttrain_data_X, train_data_Y = np.array(train_data_X)/255.0, np.array(train_data_Y)\n",
        "\tvalidation_data_X, validation_data_Y = np.array(validation_data_X)/255.0, np.array(validation_data_Y)\n",
        "\ttest_data_X, test_data_Y = np.array(test_data_X)/255.0, np.array(test_data_Y)\n",
        "\n",
        "  \n",
        "\tprint(\"train_data \", train_data_X.shape)\n",
        "\tprint(\"validation_data \", validation_data_X.shape)\n",
        "\tprint(\"test_data \", test_data_X.shape)\n",
        "\treturn train_data_X, train_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y\n",
        "\n",
        "train_data_X, train_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y = load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['emotion', 'pixels', 'Usage']\n",
            "train_data  (28709, 2304)\n",
            "validation_data  (3589, 2304)\n",
            "test_data  (3589, 2304)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QTTinxHEyGyj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resnet50"
      ]
    },
    {
      "metadata": {
        "id": "--S8Vyw_1HK3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape array\n",
        "train_data_X = train_data_X.reshape(28709, 48, 48)\n",
        "validation_data_X = validation_data_X.reshape(3589, 48, 48)\n",
        "test_data_X = test_data_X.reshape(3589, 48, 48)\n",
        "\n",
        "train_data_Y = to_categorical(train_data_Y, num_classes=7)\n",
        "validation_data_Y = to_categorical(validation_data_Y, num_classes=7)\n",
        "test_data_Y = to_categorical(test_data_Y, num_classes=7)\n",
        "\n",
        "\n",
        "IMAGE_SHAPE = (48, 48, 1)\n",
        "raw_data_csv_file_name = 'fer2013.csv'\n",
        "N_TEST = 25000\n",
        "\n",
        "# 7 Classes\n",
        "num_classes = 7\n",
        "resnetPreTrained = ResNet50(include_top=False, weights='imagenet', input_shape=(200,200,3))\n",
        "\n",
        "\n",
        "x_train_feature_map = np.empty([N_TEST, 2048])\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"loading images {}\".format(i*5000))\n",
        "    load_range = (i*5000)\n",
        "    f_l = np.empty([int(N_TEST/20), 200, 200, 3])\n",
        "    for index, item in enumerate(f_l[load_range:load_range+4999]):  # Refill the list\n",
        "        for index, item in enumerate(f_l):  # Refill the list\n",
        "            for d in range(3):\n",
        "                item[0:48, 0:48, d] = train_data_X[index]\n",
        "\n",
        "\n",
        "    picture_train_features = resnetPreTrained.predict(f_l)\n",
        "    del(f_l)\n",
        "\n",
        "    #BUILD NEW TRAIN FEATURE INPUT\n",
        "    for idx_pic, picture in enumerate(picture_train_features):\n",
        "        idx = idx_pic + (i*5000)\n",
        "        x_train_feature_map[idx] = picture[0][0]\n",
        "\n",
        "###### TEST data\n",
        "f_t = np.empty([3589, 200, 200, 3])\n",
        "for index, item in enumerate(f_t):  # Refill the list\n",
        "    for d in range(3):\n",
        "        item[0:48, 0:48,d] = test_data_X[index]\n",
        "\n",
        "\n",
        "picture_test_features = resnetPreTrained.predict(f_t)\n",
        "del(f_t)\n",
        "\n",
        "#BUILD NEW TEST\n",
        "x_test_feature_map  = np.empty([3589, 2048])\n",
        "for idx_pic, picture in enumerate(picture_test_features):\n",
        "    x_test_feature_map[idx_pic] = picture[0][0]\n",
        "    \n",
        "##### Validation data   \n",
        "f_t = np.empty([3589, 200, 200, 3])\n",
        "for index, item in enumerate(f_t):  # Refill the list\n",
        "    for d in range(3):\n",
        "        item[0:48, 0:48,d] = validation_data_X[index]\n",
        "\n",
        "\n",
        "picture_validation_features = resnetPreTrained.predict(f_t)\n",
        "del(f_t)\n",
        "\n",
        "#BUILD NEW TEST\n",
        "x_validation_feature_map  = np.empty([3589, 2048])\n",
        "for idx_pic, picture in enumerate(picture_validation_features):\n",
        "    x_validation_feature_map[idx_pic] = picture[0][0]\n",
        "\n",
        "\n",
        "print(\"here\")\n",
        "with tf.device('/gpu:0'):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(1024, input_shape=(2048,),activation='relu'))\n",
        "  model.add(Dense(512, input_shape=(1024,),activation='relu'))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  adamax = Adamax()\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=adamax, metrics=['accuracy'])\n",
        "  model.fit(x_train_feature_map, train_data_Y[0:N_TEST], validation_data=(x_validation_feature_map, validation_data_Y), epochs=10, batch_size=32)\n",
        "\n",
        "  score = model.evaluate(x_test_feature_map, test_data_Y, batch_size=32)\n",
        "  print(\"TeResult\")\n",
        "  print(score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kIHBfjB_l3aM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train on 25000 samples, validate on 3589 samples  \n",
        "\n",
        "Epoch 1/10\n",
        "25000/25000 [==============================] - 8s 320us/step - loss: 2.5333 - acc: 0.2440 - val_loss: 1.8166 - val_acc: 0.2449  \n",
        "Epoch 2/10\n",
        "25000/25000 [==============================] - 6s 251us/step - loss: 2.5313 - acc: 0.2468 - val_loss: 1.8151 - val_acc: 0.2449  \n",
        "Epoch 3/10\n",
        "25000/25000 [==============================] - 6s 250us/step - loss: 2.5302 - acc: 0.2469 - val_loss: 1.8237 - val_acc: 0.2441      \n",
        "Epoch 4/10\n",
        "25000/25000 [==============================] - 6s 250us/step - loss: 2.5300 - acc: 0.2466 - val_loss: 1.8166 - val_acc: 0.2460     \n",
        "Epoch 5/10\n",
        "25000/25000 [==============================] - 6s 252us/step - loss: 2.5299 - acc: 0.2468 - val_loss: 1.8132 - val_acc: 0.2455   \n",
        "Epoch 6/10\n",
        "25000/25000 [==============================] - 6s 251us/step - loss: 2.5294 - acc: 0.2470 - val_loss: 1.8088 - val_acc: 0.2455   \n",
        "Epoch 7/10\n",
        "25000/25000 [==============================] - 6s 251us/step - loss: 2.5292 - acc: 0.2471 - val_loss: 1.8151 - val_acc: 0.2466   \n",
        "Epoch 8/10\n",
        "25000/25000 [==============================] - 6s 251us/step - loss: 2.5294 - acc: 0.2470 - val_loss: 1.8080 - val_acc: 0.2458   \n",
        "Epoch 9/10\n",
        "25000/25000 [==============================] - 6s 253us/step - loss: 2.5291 - acc: 0.2469 - val_loss: 1.8081 - val_acc: 0.2460   \n",
        "Epoch 10/10\n",
        "25000/25000 [==============================] - 6s 251us/step - loss: 2.5289 - acc: 0.2472 - val_loss: 1.8079 - val_acc: 0.2466  \n",
        "3589/3589 [==============================] - 0s 78us/step  \n",
        "Test Result\n",
        "[loss = 1.8071214112773117,  Accuracy = 0.2471440512719145]"
      ]
    },
    {
      "metadata": {
        "id": "ZX0cOtEDca6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "def load_image(img_path, show=True):\n",
        "\n",
        "    img = image.load_img(img_path, target_size=(48, 48), color_mode = \"grayscale\")\n",
        "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
        "    img_tensor /= 255.\n",
        "    #print(img_tensor.shape)\n",
        "    if show:\n",
        "        plt.imshow(img, cmap='gray')\n",
        "\n",
        "    return img_tensor\n",
        "    # show the image\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # load model\n",
        "    #model = load_model(\"model.h5\")\n",
        "\n",
        "    # image path\n",
        "    #!wget https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyndHmil5Mdk8-v-6To_6ncm-l8-L5n0l0Bh5dcOR6ZUkq-pODmQ #Angry\n",
        "    #!wget https://www.sciencedaily.com/images/2009/03/090312093916_1_540x360.jpg #Disgust\n",
        "    #!wget https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcREqmMQoG0_0XOYf5YWhLIdo-xZmohGMOWm0P4aFUHpFzYiB_yY #Fear\n",
        "    #!wget https://w3.chabad.org/media/images/689/tyZo6894268.jpg #Happy\n",
        "    #!wget https://c1.staticflickr.com/4/3675/9436653177_fd00cc9d2c_b.jpg #Sad\n",
        "    #!wget https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTt54l_3Sud9ektJH69BXBOADn3qk7d0XrTpjrAWJ6u5omaX2NlcQ # Surprise \n",
        "    #!wget https://goldenmeancalipers.com/wp-content/uploads/2011/12/mirror11.jpg # Netural\n",
        "\n",
        "    # load a single image\n",
        "    new_image = load_image('mirror11.jpg')\n",
        "    \n",
        "    # check prediction\n",
        "    pred = model.predict(new_image)\n",
        "    index = pred.argmax()\n",
        "    print(result[index])\n",
        "    #print(max(pred))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}